m <- matrix(nrow = 3, ncol = 3)
m <- matrix(nrow = 3, ncol = 3)
m <- matrix(nrow = 3, ncol = 3)
print(m)
?matrix
m <- matrix(nrow = 3, ncol = 3)
print(m)
?matrix
m <- matrix(rbind(
c(0,0,0)
c(0,0,0)
?matrix
m <- matrix(rbind(
c(0,0,0),
c(0,0,0),
c(0,0,0)
))
print(m)
?matrix
m <- matrix(cbind(
c(0,0,0),
c(0,0,0),
c(0,0,0)
))
print(m)
?matrix
m <- matrix(0, 3,3)
print(m)
m <- matrix(0, 3,3, dimnames = list(c('A','B','C'), c('A','B','C')))
print(m)
elegans_network <- data(elegans, package = "SNA4DSData")
summary(elegans_network)
plot(elegans_network)
data(elegans, package = "SNA4DSData")
plot(data(elegans, package = "SNA4DSData"))
help(package = 'igraph')
class(network)
network <- data(elegans, package = "SNA4DSData"))
network <- data(elegans, package = "SNA4DSData")
print(class(network))
print(network)
remotes::install_github("SNAnalyst/SNA4DS", dependencies = TRUE)
remotes::install_github("SNAnalyst/SNA4DSData", dependencies = TRUE)
network <- data(elegans, package = "SNA4DSData")
help(package = 'igraph')
print(network)
class(network)
SNA4DS::SNA4DS_tutorials()
q()
igraph::as.undirected(dara)
igraph::as.undirected(data)
data <- data(elegans, package = "SNA4DSData")
help(package = 'igraph')
igraph::as.undirected(data)
igraph::as.data_frame(data)
class(elegans)
source("C:/Gergo-mappa/projects/programming/projects/jads/sna4ds/homeplay_1.R", echo=TRUE)
help(package='igraph')
igraph::dyad_census(elegans)
igraph::reciprocity(elegans)
network <- intergraph::asNetwork(elegans)
class(network)
help(package='sna')
sna::dyad.census(network)
sna::grecip(network, measure = "edgewise")
SNA4DS::SNA4DS_tutorials()
`remotes::install_github("SNAnalyst/SNA4DS", dependencies = TRUE)`
`remotes::install_github("SNAnalyst/SNA4DSData", dependencies = TRUE)`
remotes::install_github("SNAnalyst/SNA4DS", dependencies = TRUE)
SNA4DS::SNA4DS_tutorials()
SNA4DS::open_SNA4DS_vignettes()
data(blososphere, package="SNA4DSData")
remotes::install_github("SNAnalyst/SNA4DSData", dependencies = TRUE)
data(blososphere, package = "SNA4DSData")
data(blogosphere, package = "SNA4DSData")
igraph::which_loop(blogosphere)
loops <- igraph::which_loop(blogosphere)
TRUE in loops
TRUE %in% loops
simple_blogosphere <- igraph::without_loops(blogosphere)
simple_blogosphere <- igraph::simplify(blogosphere, remove.loops = TRUE)
loops_new <- igraph::which_loop(simple_blogosphere)
TRUE %in% loops_new
igraph::mean_distance(simple_blogosphere)
igraph::diameter(simple_blogosphere)
igraph::dyad.census(simple_blogosphere)
igraph::reciprocity(simple_blogosphere)
igraph::transitivity(simple_blogosphere)
igraph::edge_density(simple_blogosphere)
igraph::cluster_walktrap(simple_blogosphere)
?igraph
help(package='igraph')
for (vertice in igraph::V(simple_blogosphere)){
if ( igraph::degree(simple_blogosphere, v = vertice ) == 0){
simple_blogosphere <- igraph::delete.vertices(simple_blogosphere, vertice)
}
}
isolated <- which( igraph::degree(simple_blogosphere) == 0)
connected_blogosphere <- igraph::delete.vertices(simple_blogosphere, isolated)
igraph::cluster_walktrap(connected_blogosphere)
remotes::install_github("SNAnalyst/bootcamp_2021", dependencies=true)
remotes::install_github("SNAnalyst/bootcamp_2021", dependencies=True)
remotes::install_github("SNAnalyst/bootcamp_2021", dependencies=TRUE)
bootcamp2021::bootcamp2021_tutorials()
exit
data(centrality, package = "bootcamp21")
print(descriptives(centrality))
print(bootcamp2021::descriptives(centrality))
bootcamp2021::descriptives(centrality)
data(centrality, package = "bootcamp2021")
bootcamp2021::descriptives(centrality)
mod_1 <- lm(friendship ~ race + gender + age, data = centrality)
summary(mod_1)
bootcamp2021::betterpairs(centrality)
outliertree::outlier.tree(centrality)
mod_2 <- lm(friendship ~ conscientiousness + extraversion + neuroticism + agreeableness + openness_experience)
mod_2 <- lm(friendship ~ conscientiousness + extraversion + neuroticism + agreeableness + openness_experience, data = centrality)
summary(mod_3)
summary(mod_2)
lm(friendship ~ conscientiousness + extraversion + neuroticism + agreeableness + openness_experience + activity_preference, data = centrality)
summary(mod_3)
mod_3 <- lm(friendship ~ conscientiousness + extraversion + neuroticism + agreeableness + openness_experience + activity_preference, data = centrality)
summary(mod_3)
plot(mod_1)
plot(mod_1)
plot(mod_2)
plot(mod_2)
plot(mod_2)
plot(mod_3)
remotes::install_github("SNAnalyst/bootcamp_2021", dependencies = TRUE, force = TRUE)
bootcamp2021::bootcamp2021_tutorials()
bootcamp2021::bootcamp2021_tutorials()
data(dozen, package = "bootcamp2021")
print(dozen$three)
print(dozen$three)
print(dozen$three)
for ( i in range(1,12)) {
print(apply(dozen[[i]], 2, mean))
}
?rnage
?range
for ( i in sequence(1,12)) {
print(apply(dozen[[i]], 2, mean))
}
for ( i in sequence(1,12)) {
print(i)
print(apply(dozen[[i]], 2, mean))
}
for ( i in sequence(1,12)) {
l = c()
append(l, c(i, apply(dozen[[i]], 2, mean)))
}
print(l)
l = c()
for ( i in sequence(1,12)) {
append(l, c(i, apply(dozen[[i]], 2, mean)))
}
print(l)
for ( i in sequence(1,12)) {
l <- c(l, c(i, apply(dozen[[i]], 2, mean)))
}
print(l)
?sequnece
?sequence
for ( i in seq(from = 1, to = 12, by = 1)) {
l <- c(l, c(i, apply(dozen[[i]], 2, mean)))
}
print(l)
for ( i in seq(from = 1, to = 12, by = 1)) {
plot(x = dozen[[i]]$x, y = dozen[[i]]$y,
main = str(i),
pch = 16,
col = "blue")
abline(reg = lm(sleep$non_dreaming ~ sleep$dreaming), col = "red", lty = "dashed")
text(6.5, 15.3, label = "OLS", col = "red")
}
lapply(dozen, apply, 2, plot(
main = str(i),
pch = 16,
col = "blue"))
?cor
lapply(dozen, apply, 2, plot
lapply(dozen, apply, 2, plot)
(dozen, apply, 2, plot)
lapply(dozen, apply, 2, plot)
for ( i in seq(from = 1, to = 12, by = 1)) {
cor(x = dozen[[i]]$x, dozen[[i]]$y)
}
l = c()
for ( i in seq(from = 1, to = 12, by = 1)) {
l <- c(l, c(i, cor(x = dozen[[i]]$x, dozen[[i]]$y)))
}
print(l)
l = c()
for ( i in seq(from = 1, to = 12, by = 1)) {
l <- c(l, c(i, cor(x = dozen[[i]]$x, dozen[[i]]$y), '####'))
}
(l)
data(loans, package = 'bootcamp2021')
bootcamp2021::descriptives(loans$state)
bootcamp2021::descriptives(loans$debt_to_income)
SNA4DS::SNA4DS_tutorials()
# setting the working directory to my local computer
setwd('C:/Gergo-mappa/projects/programming/projects/jads/sbm/jads_2021_g9_sbm')
many_models = load('num_topics_df.Rda')
library(stm)
library(dplyr)
library(tidytext)
library(bigrquery)
library(purrr)
library(tidyr)
library(ggplot2)
library(furrr)
library(stminsights)
library(quanteda)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, documents=docs),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, documents=docs),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, documents=docs),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, documents=docs),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
library(stm)
library(dplyr)
library(tidytext)
library(bigrquery)
library(purrr)
library(tidyr)
library(ggplot2)
library(furrr)
library(stminsights)
library(quanteda)
setwd('C:/Gergo-mappa/projects/programming/projects/jads/sbm/jads_2021_g9_sbm')
library(stm)
library(dplyr)
library(tidytext)
library(bigrquery)
library(purrr)
library(tidyr)
library(ggplot2)
library(furrr)
library(stminsights)
library(quanteda)
('./data/clean_games_data.csv')
('./data/clean_games_data.csv')
games_data <- read.csv('./data/clean_games_data.csv')
get_word_freq <- function(dataframe, n = 25) {
word_list <- dataframe %>%
tidytext::unnest_tokens(word, clean_text)
freq_list <- word_list %>%
dplyr::count(word, sort = TRUE)
return(c(freq_list[1:n,1]))
}
get_word_freq <- function(dataframe, n = 25) {
word_list <- dataframe %>%
tidytext::unnest_tokens(word, clean_text)
freq_list <- word_list %>%
dplyr::count(word, sort = TRUE)
return(c(freq_list[1:n,1]))
}
stm_data <- games_data[c('clean_text', 'location', 'Category', 'launch_date')]
processed <- textProcessor(stm_data$clean_text, metadata = stm_data, lowercase = FALSE, removepunctuation = FALSE, stem=FALSE, sparselevel = 0.997, customstopwords = get_word_freq(stm_data))
plotRemoved(processed$documents, lower.thresh = seq(1, 350, by = 50))
tester = 'book emoji help relay going world share photo video favorite memory get notification friend like comment post find local social event make plan meet friend play game facebook friend backup photo saving album follow favorite artist website company get latest news look local business see review operation hour picture buy sell locally facebook marketplace watch live video go facebook app help stay connected friend interest also personal organizer storing saving sharing photo easy share photo straight android camera full control photo privacy setting choose keep individual photo private even set secret photo album control see facebook also help keep latest news current event around world subscribe favorite celebrity brand news'
calculate_sentiment <- function(description){
# tokenize
tokens <- data_frame(text = description) %>% unnest_tokens(word, text)
# get the sentiment from the first text:
sentiment <- tokens %>%
inner_join(get_sentiments("bing")) %>% # pull out only sentimen words
count(sentiment) %>% # count the # of positive & negative words
spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
mutate(sentiment = positive - negative) # # of positive words - # of negative owrds
# return our sentiment dataframe
return(sentiment$sentiment)
}
calculate_sentiment(tester)
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
calculate_sentiment <- function(description){
# tokenize
tokens <- data_frame(text = description) %>% unnest_tokens(word, text)
# get the sentiment from the first text:
sentiment <- tokens %>%
inner_join(get_sentiments("bing")) %>% # pull out only sentimen words
count(sentiment) %>% # count the # of positive & negative words
spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
mutate(sentiment = positive - negative) # # of positive words - # of negative owrds
# return our sentiment dataframe
return(sentiment$sentiment)
}
tester = 'book emoji help relay going world share photo video favorite memory get notification friend like comment post find local social event make plan meet friend play game facebook friend backup photo saving album follow favorite artist website company get latest news look local business see review operation hour picture buy sell locally facebook marketplace watch live video go facebook app help stay connected friend interest also personal organizer storing saving sharing photo easy share photo straight android camera full control photo privacy setting choose keep individual photo private even set secret photo album control see facebook also help keep latest news current event around world subscribe favorite celebrity brand news'
calculate_sentiment(tester)
get_sentiments('bing')
tokens <- data_frame(text = tester) %>% unnest_tokens(word, text)
tokens
inner_join(get_sentiments("bing"), tokens)
inner_join(get_sentiments("AFINN "), tokens)
inner_join(get_sentiments("afinn"), tokens)
install.packages('textdata')
inner_join(get_sentiments("afinn"), tokens)
inner_join(get_sentiments("nrc"), tokens)
tmp <- inner_join(get_sentiments("nrc"), tokens)
tmp <- inner_join(get_sentiments("afinn"), tokens)
sum(tmp$value)
sum(tmp$value)/length(tmp$value)
tmp <- inner_join( tokens, get_sentiments("afinn"))
tmp
calculate_sentiment <- function(description){
# tokenize
tokens <- data_frame(text = description) %>% unnest_tokens(word, text)
# get the sentiment from the first text:
sentiment <- tokens %>%
inner_join(get_sentiments("bing"))
tmp <- inner_join( tokens, get_sentiments("afinn"))
return(sum(tmp$value)/length(tmp$value))
}
tester = 'book emoji help relay going world share photo video favorite memory get notification friend like comment post find local social event make plan meet friend play game facebook friend backup photo saving album follow favorite artist website company get latest news look local business see review operation hour picture buy sell locally facebook marketplace watch live video go facebook app help stay connected friend interest also personal organizer storing saving sharing photo easy share photo straight android camera full control photo privacy setting choose keep individual photo private even set secret photo album control see facebook also help keep latest news current event around world subscribe favorite celebrity brand news'
calculate_sentiment(tester)
calculate_sentiment <- function(description){
# tokenize
tokens <- data_frame(text = description) %>% unnest_tokens(word, text)
# get the sentiment from the first text:
sentiment <- tokens %>%
tmp <- inner_join( tokens, get_sentiments("afinn"))
return(sum(tmp$value)/length(tmp$value))
}
tester = 'book emoji help relay going world share photo video favorite memory get notification friend like comment post find local social event make plan meet friend play game facebook friend backup photo saving album follow favorite artist website company get latest news look local business see review operation hour picture buy sell locally facebook marketplace watch live video go facebook app help stay connected friend interest also personal organizer storing saving sharing photo easy share photo straight android camera full control photo privacy setting choose keep individual photo private even set secret photo album control see facebook also help keep latest news current event around world subscribe favorite celebrity brand news'
calculate_sentiment(tester)
calculate_sentiment <- function(description){
# tokenize
tokens <- data_frame(text = description) %>% unnest_tokens(word, text)
tmp <- inner_join( tokens, get_sentiments("afinn"))
return(sum(tmp$value)/length(tmp$value))
}
tester = 'book emoji help relay going world share photo video favorite memory get notification friend like comment post find local social event make plan meet friend play game facebook friend backup photo saving album follow favorite artist website company get latest news look local business see review operation hour picture buy sell locally facebook marketplace watch live video go facebook app help stay connected friend interest also personal organizer storing saving sharing photo easy share photo straight android camera full control photo privacy setting choose keep individual photo private even set secret photo album control see facebook also help keep latest news current event around world subscribe favorite celebrity brand news'
calculate_sentiment(tester)
tokens <- data_frame(text = tester) %>% unnest_tokens(word, text)
tokens
get_sentiments('afinn')
tmp <- inner_join( tokens, get_sentiments("afinn"))
tmp
return(mean(tmp$value))
calculate_sentiment <- function(description){
# tokenize
tokens <- data_frame(text = description) %>% unnest_tokens(word, text)
tmp <- inner_join( tokens, get_sentiments("afinn"))
return(mean(tmp$value))
}
calculate_sentiment(tester)
# setting the working directory to my local computer
setwd('C:/Gergo-mappa/projects/programming/projects/jads/sbm/jads_2021_g9_sbm')
library(stm)
library(dplyr)
library(tidytext)
library(bigrquery)
library(purrr)
library(tidyr)
library(ggplot2)
library(furrr)
library(stminsights)
library(quanteda)
games_data <- read.csv('./data/clean_games_data.csv')
get_word_freq <- function(dataframe, n = 25) {
word_list <- dataframe %>%
tidytext::unnest_tokens(word, clean_text)
freq_list <- word_list %>%
dplyr::count(word, sort = TRUE)
return(c(freq_list[1:n,1]))
}
stm_data <- games_data[c('clean_text', 'location', 'Category', 'launch_date')]
processed <- textProcessor(stm_data$clean_text, metadata = stm_data, lowercase = FALSE, removepunctuation = FALSE, stem=FALSE, sparselevel = 0.997, customstopwords = get_word_freq(stm_data))
plotRemoved(processed$documents, lower.thresh = seq(1, 350, by = 50))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 100, upper.thresh = 2800)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
print(paste("First heuristic to determine the number of topics:",round(sqrt(length(games_data$clean_text)/2),0)))
many_models <- data_frame(K = c(20, 50, 70, 90, 100, 125, 150,200)) %>%
mutate(topic_model = furrr::future_map(K, ~stm::stm(documents=docs, vocab=vocab, K = ., max.em.its = 10,
verbose = FALSE)))
many_models = load('num_topics_df.Rda')
heldout <- make.heldout(documents=docs, vocab=vocab)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, documents=docs),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, documents=docs),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
many_models <- data_frame(K = c(20, 50, 100, 150,)) %>%
mutate(topic_model = furrr::future_map(K, ~stm::stm(documents=docs, vocab=vocab, K = ., max.em.its = 10,
verbose = FALSE)))
many_models <- data_frame(K = c(20, 50, 100, 150)) %>%
mutate(topic_model = furrr::future_map(K, ~stm::stm(documents=docs, vocab=vocab, K = ., max.em.its = 10,
verbose = FALSE)))
heldout <- make.heldout(documents=docs, vocab=vocab)
k_result <- many_models %>%
mutate(exclusivity = map(topic_model, exclusivity),
semantic_coherence = map(topic_model, semanticCoherence, documents=docs),
eval_heldout = map(topic_model, eval.heldout, heldout$missing),
residual = map(topic_model, checkResiduals, documents=docs),
bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
lbound = bound + lfact,
iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
k_result %>%
transmute(K,
`Lower bound` = lbound,
Residuals = map_dbl(residual, "dispersion"),
`Semantic coherence` = map_dbl(semantic_coherence, mean),
`Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x = "K (number of topics)",
y = NULL,
title = "Model diagnostics by number of topics",
subtitle = "These diagnostics indicate that a good number of topics would be a number before 100")
k_result %>%
transmute(K,
`Lower bound` = lbound,
Residuals = map_dbl(residual, "dispersion"),
`Semantic coherence` = map_dbl(semantic_coherence, mean),
`Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x = "K (number of topics)",
y = NULL,
title = "Model diagnostics by number of topics",
subtitle = "These diagnostics indicate that a good number of topics would be a number around 100")
k_result %>%
select(K, exclusivity, semantic_coherence) %>%
filter(K %in% c(10,20,70,100)) %>%
unnest() %>%
mutate(K = as.factor(K)) %>%
ggplot(aes(semantic_coherence, exclusivity, color = K)) +
geom_point(size = 2, alpha = 0.7) +
labs(x = "Semantic coherence",
y = "Exclusivity",
title = "Comparing exclusivity and semantic coherence",
subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
k_result %>%
select(K, exclusivity, semantic_coherence) %>%
filter(K %in% c(20, 50, 100, 150)) %>%
unnest() %>%
mutate(K = as.factor(K)) %>%
ggplot(aes(semantic_coherence, exclusivity, color = K)) +
geom_point(size = 2, alpha = 0.7) +
labs(x = "Semantic coherence",
y = "Exclusivity",
title = "Comparing exclusivity and semantic coherence",
subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
games_test_fit <- stm(documents = out$documents, vocab = out$vocab, K = 100, prevalence =~ location + Category + launch_date, max.em.its = 20, reportevery = 5, data = out$meta, init.type = "Spectral")
k_result %>%
transmute(K,
`Lower bound` = lbound,
Residuals = map_dbl(residual, "dispersion"),
`Semantic coherence` = map_dbl(semantic_coherence, mean),
`Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
gather(Metric, Value, -K) %>%
ggplot(aes(K, Value, color = Metric)) +
geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(~Metric, scales = "free_y") +
labs(x = "K (number of topics)",
y = NULL,
title = "Model diagnostics by number of topics",
subtitle = "These diagnostics indicate that a good number of topics would be a number around 50")
games_test_fit <- stm(documents = out$documents, vocab = out$vocab, K = 50, prevalence =~ location + Category + launch_date, max.em.its = 20, reportevery = 5, data = out$meta, init.type = "Spectral")
